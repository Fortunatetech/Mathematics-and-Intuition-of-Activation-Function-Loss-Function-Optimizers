### Math and Intuition of Activation Functions, Loss Functions, and Optimizers

This repository provides a comprehensive collection of resources explaining the mathematical concepts and intuitive explanations behind activation functions, loss functions,
and optimizers commonly used in machine learning and deep learning.

Activation functions play a crucial role in introducing non-linearity to neural networks, and this repository covers popular activation functions such as
Sigmod activation Function, Tahn activation function, ReLU etc, providing insights into their mathematical formulations and intuitive interpretations.

Loss functions are essential for evaluating the performance of machine learning models. This repository explores various loss functions, including
MSE, MAE, Huber Loss, Cross Entropy etc, explaining their mathematical foundations and practical implications.

Optimizers are algorithms used to update the model's parameters during the training process. The repository delves into optimizers like 
Gradient Descent, Mini Barch SaD, Adam Optimizer etc, shedding light on their inner workings and guiding principles.

Each section of the repository contains detailed explanations, intuitive insights, and mathematical derivations, enabling a deeper understanding of these 
key components in machine learning.

Contributions to this repository are encouraged! If you have additional insights, alternative explanations, or new activation functions, loss functions, or optimizers to share, please feel free to contribute by submitting a pull request.

Explore the resources and enhance your understanding of activation functions, loss functions, and optimizers in machine learning!

